{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "seed = 161\n",
    "# random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(seed);\n",
    "\n",
    "N_ACTIONS   = env.action_space.n\n",
    "N_STATES    = env.observation_space.shape[0]\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape\n",
    "\n",
    "def discretize(c_pos_val, c_vel_val, p_ang_val, p_vel_val, c_pos_s, c_vel_s, p_ang_s, p_vel_s):\n",
    "    c_pos_indx = np.where(c_pos_s >= c_pos_val)[0][0].astype(int)\n",
    "    c_vel_indx = np.where(c_vel_s >= c_vel_val)[0][0].astype(int)\n",
    "    p_ang_indx = np.where(p_ang_s >= p_ang_val)[0][0].astype(int)\n",
    "    p_vel_indx = np.where(p_vel_s >= p_vel_val)[0][0].astype(int)\n",
    "    return [c_pos_indx, c_vel_indx, p_ang_indx, p_vel_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization Level:  25\n"
     ]
    }
   ],
   "source": [
    "QUANTIZATION_LEVEL = 25\n",
    "print(\"Quantization Level: \", QUANTIZATION_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_TABLE = np.zeros((QUANTIZATION_LEVEL,\n",
    "                    QUANTIZATION_LEVEL,\n",
    "                    QUANTIZATION_LEVEL,\n",
    "                    QUANTIZATION_LEVEL, \n",
    "                    N_ACTIONS))\n",
    "\n",
    "D_Q_TABLE = Q_TABLE\n",
    "\n",
    "c_pos_s = np.linspace(-5, 5, QUANTIZATION_LEVEL)\n",
    "c_vel_s = np.linspace(-5, 5, QUANTIZATION_LEVEL)\n",
    "p_ang_s = np.linspace(-1, 1, QUANTIZATION_LEVEL)\n",
    "p_vel_s = np.linspace(-5, 5, QUANTIZATION_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_LR           = 1e-1\n",
    "T_GAMMA        = 0.9\n",
    "T_EPSILON      = 0.9\n",
    "\n",
    "NO_OF_NODES    = 50\n",
    "NO_OF_EPISODES = 20\n",
    "TIMESTEP_LIMIT = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE          = 32\n",
    "LR                  = 1e-3  # learning rate\n",
    "EPSILON             = 0.9   # greedy policy\n",
    "GAMMA               = 0.9   # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "SEGREGATION_BIAS    = 6     # no. of non-terminal memories in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_OF_ITERATIONS = 10\n",
    "NN_ITERATIONS    = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS   = env.action_space.n\n",
    "N_STATES    = env.observation_space.shape[0]\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 50)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)   # initialization\n",
    "        self.out = nn.Linear(50, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.out.weight)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "\n",
    "        self.learn_step_counter  = 0 # for target updating\n",
    "        \n",
    "        self.good_memory_counter = 0 # for storing non-terminal memories\n",
    "        self.good_memory         = []#np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.bad_memory_counter  = 0 # for storing terminal memories\n",
    "        self.bad_memory          = []#np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.optimizer           = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func           = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def get_qvals(self,x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        actions_value = actions_value.data.numpy()\n",
    "        return actions_value\n",
    "    \n",
    "#     def store_transition(self, s, a, r, s_):\n",
    "#         transition = np.hstack((s, [a, r], s_))\n",
    "#         if r > 0: #non-terminal rewards\n",
    "#             # replace the old memory with new memory\n",
    "#             index = self.good_memory_counter % int(MEMORY_CAPACITY/2)\n",
    "#             self.good_memory[index, :] = transition\n",
    "#             self.good_memory_counter += 1\n",
    "        \n",
    "#         else: #terminal rewards\n",
    "#             # replace the old memory with new memory\n",
    "#             index = self.bad_memory_counter % int(MEMORY_CAPACITY/2)\n",
    "#             self.bad_memory[index, :] = transition\n",
    "#             self.bad_memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        good_sample_index = np.random.choice(int(self.good_memory.shape[0]), int(SEGREGATION_BIAS))\n",
    "        bad_sample_index  = np.random.choice(int(self.bad_memory.shape[0]), int(BATCH_SIZE-SEGREGATION_BIAS))\n",
    "\n",
    "        b_good_memory = self.good_memory[good_sample_index, :]\n",
    "        b_bad_memory  = self.bad_memory[bad_sample_index, :]\n",
    "        b_memory      = np.vstack((b_good_memory,b_bad_memory))\n",
    "        \n",
    "        b_s  = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a  = torch.LongTensor( b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r  = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval   = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next   = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss     = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ITERATION # 0\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    print(\"\\n\")\n",
    "    print(\"ITERATION #\", iteration)\n",
    "    tic = datetime.now()\n",
    "    node_time_rec = np.zeros((NO_OF_NODES, NO_OF_EPISODES))\n",
    "    exp_rec = np.empty(N_STATES * 2 + 2) # will contain experiences of all nodes for all episodes\n",
    "\n",
    "    for i_node in range(NO_OF_NODES):\n",
    "        time_rec = []\n",
    "        level_up_flag    = False\n",
    "        level_up_counter = 0\n",
    "        level_up_metric  = 195\n",
    "        for i_episode in range(NO_OF_EPISODES):\n",
    "            time_steps = 0\n",
    "            s = env.reset()\n",
    "            while True:\n",
    "        #         env.render()\n",
    "                [c_pos_state, \n",
    "                c_vel_state, \n",
    "                p_ang_state, \n",
    "                p_vel_state] = discretize(s[0],s[1],s[2],s[3],\n",
    "                                         c_pos_s, \n",
    "                                         c_vel_s, \n",
    "                                         p_ang_s, \n",
    "                                         p_vel_s)\n",
    "                time_steps += 1\n",
    "                if np.random.uniform() > T_EPSILON:   # greedy\n",
    "                    a = np.random.randint(0, N_ACTIONS)\n",
    "                else:\n",
    "                    a = Q_TABLE[c_pos_state, c_vel_state, p_ang_state, p_vel_state, :].argmax()\n",
    "\n",
    "                 # take action\n",
    "                s_, r, done, info = env.step(a)\n",
    "\n",
    "                if done:\n",
    "                    r = -1\n",
    "                    if time_steps >= TIMESTEP_LIMIT:\n",
    "                        r = 1\n",
    "\n",
    "                experience = np.hstack((s,a,r,s_))\n",
    "                exp_rec = np.vstack((exp_rec, experience))\n",
    "\n",
    "                #discretize next_state\n",
    "                [next_c_pos_state, \n",
    "                next_c_vel_state, \n",
    "                next_p_ang_state, \n",
    "                next_p_vel_state] = discretize(s_[0],    s_[1],    s_[2],    s_[3],\n",
    "                                              c_pos_s,  c_vel_s,  p_ang_s,  p_vel_s)\n",
    "\n",
    "                # learn\n",
    "                this_state = tuple([c_pos_state, \n",
    "                              c_vel_state, \n",
    "                              p_ang_state, \n",
    "                              p_vel_state])\n",
    "\n",
    "                next_state = tuple([ next_c_pos_state, \n",
    "                               next_c_vel_state, \n",
    "                               next_p_ang_state, \n",
    "                               next_p_vel_state])\n",
    "\n",
    "                Q_TABLE[this_state][a] = Q_TABLE[this_state][a] + T_LR * (r + T_GAMMA * Q_TABLE[next_state].max() - \n",
    "                                                                         Q_TABLE[this_state][a])\n",
    "                if done:\n",
    "                    node_time_rec[i_node][i_episode] = time_steps\n",
    "                    break\n",
    "                s = s_\n",
    "            if np.mean(node_time_rec[i_node][-10:]) > level_up_metric:\n",
    "                level_up_metric_counter += 1\n",
    "            else:\n",
    "                level_up_metric_counter = 0\n",
    "\n",
    "            if level_up_metric_counter > 10:\n",
    "                level_up_metric_counter = 0\n",
    "                T_LR *= 0.1\n",
    "                if T_EPSILON < 0.99:\n",
    "                    T_EPSILON += 0.01  \n",
    "\n",
    "    print(\"TIME TO GATHER {:d} experiences:{}\".format(NO_OF_NODES*NO_OF_EPISODES, (datetime.now()-tic)))\n",
    "    exp_rec = np.delete(exp_rec, 0, 0)\n",
    "    #PLOT EXPERIENCES\n",
    "    node_avg_time = node_time_rec.mean(axis=1)\n",
    "    node_std_time = node_time_rec.std(axis=1)\n",
    "    node_max_time = node_time_rec.max(axis=1)\n",
    "    node_min_time = node_time_rec.min(axis=1)\n",
    "\n",
    "    fig = plt.figure(figsize = (15,3))\n",
    "    ax2 = fig.add_subplot(1, 1, 1)\n",
    "    ax2.set_title(\"Q-table Performance\")\n",
    "    ax2.bar(range(NO_OF_NODES) , node_max_time, alpha = 0.1, color = 'r', edgecolor = 'black', capsize=7 )\n",
    "    ax2.bar(range(NO_OF_NODES) , node_avg_time, alpha = 0.5, color = 'g', edgecolor = 'black', capsize=7 )\n",
    "    ax2.bar(range(NO_OF_NODES) , node_min_time, alpha = 0.4, color = 'r', edgecolor = 'black', capsize=7 )\n",
    "\n",
    "    ax2.plot(np.ones_like(node_avg_time)*200, 'g--')\n",
    "    ax2.set_ylabel('Mean Node Lifetime',color = 'g')\n",
    "    ax2.set_ylim(0,TIMESTEP_LIMIT+10)\n",
    "    fig.tight_layout()\n",
    "    ax2.grid()\n",
    "    plt.show()\n",
    "\n",
    "    #GATHER ALL EXPERIENCES\n",
    "    all_exp = exp_rec\n",
    "\n",
    "    #shuffle experiences\n",
    "    np.random.shuffle(all_exp)\n",
    "\n",
    "    #segregate experiences\n",
    "    dqn.good_memory = all_exp[all_exp[:,5] == 1]\n",
    "    dqn.bad_memory = all_exp[all_exp[:,5] < 1]\n",
    "\n",
    "    #learn\n",
    "    print(\"Training Neural Network for \", NN_ITERATIONS, \"iterations\")\n",
    "    tic=datetime.now()\n",
    "    level_up_metric = 0\n",
    "    for nn_iter in range(NN_ITERATIONS):\n",
    "        dqn.learn()\n",
    "        #validate by running for TIMESTEP_LIMIT iterations\n",
    "        if(nn_iter%int(NN_ITERATIONS/5) == int(NN_ITERATIONS/5)-1):\n",
    "            print(\"Validating\", end='\\t')\n",
    "            time_rec = []\n",
    "            for i_episode in range(TIMESTEP_LIMIT):\n",
    "                time_step = 0\n",
    "                s = env.reset()\n",
    "                while True:\n",
    "                    time_step += 1 \n",
    "                    a = dqn.choose_greedy_action(s)\n",
    "                    s_, r, done, info = env.step(a)\n",
    "                    if done:\n",
    "                        break\n",
    "                    s = s_\n",
    "                time_rec = np.append(time_rec, time_step)\n",
    "            mean_time = time_rec.mean()\n",
    "            print(\"MEAN TIME: \", mean_time)\n",
    "            if mean_time >= level_up_metric:\n",
    "                level_up_metric = mean_time\n",
    "                torch.save(dqn.eval_net.state_dict(), 'best_model_'+str(QUANTIZATION_LEVEL))\n",
    "\n",
    "    print(\"TRAINING TIME:{}\".format(datetime.now()-tic))\n",
    "\n",
    "\n",
    "    dqn.eval_net.load_state_dict(torch.load('best_model_'+str(QUANTIZATION_LEVEL)))\n",
    "    dqn.eval_net.eval()\n",
    "    \n",
    "    #test NN policy\n",
    "    time_rec = []\n",
    "    for i_episode in range(TIMESTEP_LIMIT):\n",
    "        time_step = 0\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "    #         env.render()\n",
    "            time_step += 1 \n",
    "            a = dqn.choose_greedy_action(s)\n",
    "            s_, r, done, info = env.step(a)\n",
    "            if done:\n",
    "                break\n",
    "            s = s_\n",
    "        time_rec = np.append(time_rec, time_step)\n",
    "\n",
    "    fig = plt.figure(figsize = (15,3))\n",
    "    ax2 = fig.add_subplot(1, 1, 1)\n",
    "    data = time_rec\n",
    "    ax2.plot(data, color = 'm')\n",
    "    ax2.plot(np.ones_like(data)*200, 'm--')\n",
    "    ax2.set_title('Neural Network Performance')\n",
    "    ax2.set_ylabel('Time Steps',color = 'm')\n",
    "    ax2.set_ylim(0,TIMESTEP_LIMIT+10)\n",
    "    fig.tight_layout()\n",
    "    ax2.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Discretize the Q-Value Functions\n",
    "    print(\"Discretizing Q-Value Function\")\n",
    "    tic=datetime.now()\n",
    "    c_pos_s = np.linspace(-5, 5, QUANTIZATION_LEVEL)\n",
    "    c_vel_s = np.linspace(-5, 5, QUANTIZATION_LEVEL)\n",
    "    p_ang_s = np.linspace(-1, 1, QUANTIZATION_LEVEL)\n",
    "    p_vel_s = np.linspace(-5, 5, QUANTIZATION_LEVEL)\n",
    "\n",
    "\n",
    "    for c_pos_indx in np.arange(0,c_pos_s.size):\n",
    "        for c_vel_indx in np.arange(0,c_vel_s.size):\n",
    "            for p_ang_indx in np.arange(0,p_ang_s.size):\n",
    "                for p_vel_indx in np.arange(0,p_vel_s.size):\n",
    "\n",
    "                        state = [c_pos_s[c_pos_indx],\n",
    "                                 c_vel_s[c_vel_indx],\n",
    "                                 p_ang_s[p_ang_indx],\n",
    "                                 p_vel_s[p_vel_indx]]\n",
    "\n",
    "                        D_Q_TABLE[c_pos_indx,\n",
    "                                c_vel_indx,\n",
    "                                p_ang_indx,\n",
    "                                p_vel_indx, :] = dqn.get_qvals(state)\n",
    "    print(\"DISCRETIZATION TIME:{}\".format(datetime.now()-tic))\n",
    "\n",
    "    #stretch both Q_TABLE and D_Q_TABLE to [-1,1]\n",
    "    pos = D_Q_TABLE/D_Q_TABLE.max()\n",
    "    pos[pos<0] = 0\n",
    "    neg = D_Q_TABLE/np.abs(D_Q_TABLE.min())\n",
    "    neg[neg>0] = 0\n",
    "    D_Q_TABLE = pos + neg\n",
    "    \n",
    "    pos = Q_TABLE/Q_TABLE.max()\n",
    "    pos[pos<0] = 0\n",
    "    neg = Q_TABLE/np.abs(Q_TABLE.min())\n",
    "    neg[neg>0] = 0\n",
    "    Q_TABLE = pos + neg\n",
    "      \n",
    "    #NORMALIZE Q-TABLE AND UPDATE\n",
    "    Q_TABLE   = Q_TABLE + 0.7*(D_Q_TABLE-Q_TABLE)\n",
    "\n",
    "    print(\"Q-Table Updated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
